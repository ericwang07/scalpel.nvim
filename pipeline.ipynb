{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "359ee816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CMAKE_ARGS=\"-DLLAMA_METAL=ON \\\n",
    "#   -DLLAMA_METAL_EMBED_LIBRARY=ON \\\n",
    "#   -DCMAKE_BUILD_TYPE=Release \\\n",
    "#   -DLLAMA_NATIVE=ON \\\n",
    "#   -DLLAMA_ACCELERATE=ON \\\n",
    "#   -DCMAKE_C_FLAGS='-O3 -march=native -mtune=native -ffast-math' \\\n",
    "#   -DCMAKE_CXX_FLAGS='-O3 -march=native -mtune=native -ffast-math'\"\n",
    "\n",
    "# %pip install llama-cpp-python --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf5788",
   "metadata": {},
   "source": [
    "# LSP Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ec7ce96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class LSPClient:\n",
    "    def __init__(self, cmd: List[str]):\n",
    "        \"\"\"Start LSP server with given command (e.g., ['pylsp'])\"\"\"\n",
    "        self.process = subprocess.Popen(\n",
    "            cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.PIPE, text=True, bufsize=0\n",
    "        )\n",
    "        self.req_id = 0\n",
    "        time.sleep(0.1)  # Give server time to start\n",
    "        self._init_lsp()\n",
    "        \n",
    "    def _init_lsp(self):\n",
    "        \"\"\"Initialize LSP connection\"\"\"\n",
    "        response = self._send_request(\"initialize\", {\n",
    "            \"processId\": os.getpid(),\n",
    "            \"capabilities\": {\n",
    "                \"textDocument\": {\n",
    "                    \"completion\": {\"completionItem\": {\"snippetSupport\": False}}\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        if response:\n",
    "            self._send_notification(\"initialized\", {})\n",
    "    \n",
    "    def _read_message(self) -> Optional[dict]:\n",
    "        \"\"\"Read one LSP message\"\"\"\n",
    "        try:\n",
    "            # Read headers\n",
    "            content_length = 0\n",
    "            while True:\n",
    "                line = self.process.stdout.readline()\n",
    "                if not line:\n",
    "                    return None\n",
    "                if line.startswith(\"Content-Length:\"):\n",
    "                    content_length = int(line.split(\":\")[1].strip())\n",
    "                elif line.strip() == \"\":\n",
    "                    break  # End of headers\n",
    "            \n",
    "            if content_length == 0:\n",
    "                return None\n",
    "                \n",
    "            # Read content\n",
    "            content = self.process.stdout.read(content_length)\n",
    "            return json.loads(content)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _send_notification(self, method: str, params: dict):\n",
    "        \"\"\"Send notification (no response expected)\"\"\"\n",
    "        msg = json.dumps({\n",
    "            \"jsonrpc\": \"2.0\", \n",
    "            \"method\": method, \n",
    "            \"params\": params\n",
    "        })\n",
    "        full_msg = f\"Content-Length: {len(msg)}\\r\\n\\r\\n{msg}\"\n",
    "        self.process.stdin.write(full_msg)\n",
    "        self.process.stdin.flush()\n",
    "    \n",
    "    def _send_request(self, method: str, params: dict) -> Optional[dict]:\n",
    "        \"\"\"Send request and wait for response\"\"\"\n",
    "        self.req_id += 1\n",
    "        request_id = self.req_id\n",
    "        \n",
    "        msg = json.dumps({\n",
    "            \"jsonrpc\": \"2.0\", \n",
    "            \"id\": request_id, \n",
    "            \"method\": method, \n",
    "            \"params\": params\n",
    "        })\n",
    "        \n",
    "        full_msg = f\"Content-Length: {len(msg)}\\r\\n\\r\\n{msg}\"\n",
    "        self.process.stdin.write(full_msg)\n",
    "        self.process.stdin.flush()\n",
    "        \n",
    "        # Read messages until we get our response\n",
    "        for _ in range(10):  # Max 10 attempts\n",
    "            message = self._read_message()\n",
    "            if message and message.get(\"id\") == request_id:\n",
    "                return message\n",
    "            time.sleep(0.01)  # Small delay\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _collect_diagnostics(self, uri: str, timeout: float = 1.0) -> list:\n",
    "        \"\"\"Collect diagnostics for a given file URI (from publishDiagnostics).\"\"\"\n",
    "        diagnostics = []\n",
    "        start = time.time()\n",
    "\n",
    "        while time.time() - start < timeout:\n",
    "            msg = self._read_message()\n",
    "            if not msg:\n",
    "                continue\n",
    "\n",
    "            # Only care about publishDiagnostics notifications\n",
    "            if msg.get(\"method\") == \"textDocument/publishDiagnostics\":\n",
    "                params = msg.get(\"params\", {})\n",
    "                if params.get(\"uri\") == uri:\n",
    "                    diagnostics.extend(params.get(\"diagnostics\", []))\n",
    "                    # Most LSPs send one diagnostics batch per change â€” can break early\n",
    "                    break\n",
    "\n",
    "            # Give other messages a chance to arrive\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        return diagnostics\n",
    "    \n",
    "    def validate_code(self, code: str) -> bool:        \n",
    "        \n",
    "        # Create temp file\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "            f.write(code)\n",
    "            uri = f\"file://{f.name}\"\n",
    "        \n",
    "        try:\n",
    "            # Open document in LSP\n",
    "            self._send_notification(\"textDocument/didOpen\", {\n",
    "                \"textDocument\": {\n",
    "                    \"uri\": uri,\n",
    "                    \"languageId\": \"python\",\n",
    "                    \"version\": 1,\n",
    "                    \"text\": code\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            # Collect diagnostics\n",
    "            diagnostics = self._collect_diagnostics(uri)\n",
    "            \n",
    "            # Close document\n",
    "            self._send_notification(\"textDocument/didClose\", {\n",
    "                \"textDocument\": {\"uri\": uri}\n",
    "            })\n",
    "            \n",
    "            # Analyze diagnostics\n",
    "            semantic_errors = []\n",
    "            for d in diagnostics:\n",
    "                msg = d.get(\"message\", \"\").lower()\n",
    "                \n",
    "                # Ignore incomplete syntax or formatting issues\n",
    "                if any(s in msg for s in [\n",
    "                    \"unexpected eof\",\n",
    "                    \"was never closed\",\n",
    "                    \"expected\",\n",
    "                    \"newline at end\",\n",
    "                    \"indentation\",\n",
    "                ]):\n",
    "                    continue\n",
    "                \n",
    "                # Flag semantic problems\n",
    "                if any(s in msg for s in [\n",
    "                    \"undefined name\",\n",
    "                    \"attributeerror\",\n",
    "                    \"keyerror\",\n",
    "                    \"name is not defined\",\n",
    "                    \"object has no attribute\",\n",
    "                    \"not callable\",\n",
    "                    \"cannot import\",\n",
    "                ]):\n",
    "                    semantic_errors.append({\n",
    "                        'message': d.get('message'),\n",
    "                        'severity': d.get('severity'),\n",
    "                        'range': d.get('range')\n",
    "                    })\n",
    "            \n",
    "            \n",
    "            # Determine if valid\n",
    "            is_valid = len(semantic_errors) == 0  \n",
    "\n",
    "            # return {\n",
    "            #     'is_valid': is_valid,\n",
    "            #     'match': match,\n",
    "            #     'diagnostics': diagnostics,\n",
    "            #     'semantic_errors': semantic_errors\n",
    "            # }          \n",
    "            \n",
    "            return is_valid\n",
    "            \n",
    "        finally:\n",
    "            os.unlink(f.name)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cleanup\"\"\"\n",
    "        try:\n",
    "            self._send_request(\"shutdown\", {})\n",
    "            self._send_notification(\"exit\", {})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.process.terminate()\n",
    "        self.process.wait(timeout=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6d7b9",
   "metadata": {},
   "source": [
    "# Small Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "92ae3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
    "import math\n",
    "\n",
    "class LocalCodeModel:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_path: str,\n",
    "        # Model configuration parameters\n",
    "        n_ctx: int = 2048,\n",
    "        n_gpu_layers: int = -1,\n",
    "        n_threads: int = 4,\n",
    "        n_batch: int = 1024,\n",
    "        verbose: bool = False,\n",
    "        use_mlock: bool = True,\n",
    "        use_mmap: bool = True,\n",
    "        logits_all: bool = True,\n",
    "        # Generation default parameters\n",
    "        max_tokens: int = 1,\n",
    "        temperature: float = 0,\n",
    "        top_p: float = 1,\n",
    "        top_k: int = 1,\n",
    "        stop_tokens: List[str] = []\n",
    "    ):\n",
    "\n",
    "        self.model = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=n_ctx,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            n_threads=n_threads,\n",
    "            n_batch=n_batch,\n",
    "            verbose=verbose,\n",
    "            use_mlock=use_mlock,\n",
    "            use_mmap=use_mmap,\n",
    "            logits_all=logits_all,            \n",
    "            # draft_model=LlamaPromptLookupDecoding(num_pred_tokens=max_tokens)\n",
    "        )\n",
    "\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.top_k = top_k\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.n_ctx = n_ctx\n",
    "        self.token_budget = self.calculate_token_budget()\n",
    "        \n",
    "    def calculate_token_budget(self):\n",
    "        # Reserve tokens for generation\n",
    "        reserved_for_generation = self.max_tokens\n",
    "        \n",
    "        # Reserve for special FIM tokens: <|fim_prefix|>, <|fim_suffix|>, <|fim_middle|>\n",
    "        reserved_for_special_tokens = 30  # Conservative estimate\n",
    "        \n",
    "        # Safety buffer\n",
    "        safety_buffer = 10\n",
    "        \n",
    "        # Calculate available tokens for input\n",
    "        available_input_tokens = (\n",
    "            self.n_ctx - \n",
    "            reserved_for_generation - \n",
    "            reserved_for_special_tokens - \n",
    "            safety_buffer\n",
    "        )\n",
    "        \n",
    "        # Estimate chars per token (conservative for code)\n",
    "        chars_per_token = 2.5\n",
    "        \n",
    "        # Calculate available characters\n",
    "        available_chars = int(available_input_tokens * chars_per_token)\n",
    "        print(f\"Token budget: {available_input_tokens} tokens (~{available_chars} chars) available for input\")\n",
    "\n",
    "        return available_chars\n",
    "\n",
    "    def truncate_context(\n",
    "        self, \n",
    "        code_before: str, \n",
    "        code_after: str,\n",
    "        split_ratio: float = 0.75\n",
    "    ) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Truncate code_before and code_after to fit within token budget.\n",
    "        \n",
    "        Args:\n",
    "            code_before: Code before the cursor\n",
    "            code_after: Code after the cursor\n",
    "            split_ratio: Ratio of budget to allocate to code_before (0.75 = 75% before, 25% after)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (truncated_code_before, truncated_code_after)\n",
    "        \"\"\"\n",
    "        # Calculate max characters for each part\n",
    "        max_before_chars = int(self.token_budget * split_ratio)\n",
    "        max_after_chars = int(self.token_budget * (1 - split_ratio))\n",
    "        \n",
    "        # Truncate: keep END of before (most recent), START of after (immediate next)\n",
    "        truncated_before = code_before[-max_before_chars:] if len(code_before) > max_before_chars else code_before\n",
    "        truncated_after = code_after[:max_after_chars] if len(code_after) > max_after_chars else code_after\n",
    "        \n",
    "        return truncated_before, truncated_after\n",
    "\n",
    "    def build_fim_prompt(self, code_before: str, code_after: str) -> str:\n",
    "        \"\"\"Build FIM prompt from code context.\"\"\"\n",
    "        # return f\"<|fim_begin|>{code_before}<|fim_hole|>{code_after}<|fim_end|>\"\n",
    "        return f\"<|fim_prefix|>{code_before}<|fim_suffix|>{code_after}<|fim_middle|>\"\n",
    "    \n",
    "    def clean_completion(self, completion: str, stop_chars: list[str]) -> str:\n",
    "        \"\"\"Remove trailing stop characters that slipped through.\"\"\"\n",
    "        cleaned = completion\n",
    "        \n",
    "        # Strip any stop characters from the end\n",
    "        while cleaned and cleaned[-1] in stop_chars:\n",
    "            cleaned = cleaned[:-1]\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def generate(self, code_before: str, code_after: str):\n",
    "        \n",
    "        trunc_code_before, trunc_code_after = self.truncate_context(code_before=code_before, code_after=code_after)\n",
    "        prompt = self.build_fim_prompt(code_before=trunc_code_before, code_after=trunc_code_after)\n",
    "\n",
    "        output = self.model(\n",
    "            prompt=prompt,\n",
    "            max_tokens=self.max_tokens,       \n",
    "            temperature=self.temperature,      \n",
    "            top_p=self.top_p,            \n",
    "            top_k=self.top_k,          \n",
    "            stop=self.stop_tokens,\n",
    "            logprobs=1\n",
    "        )\n",
    "       \n",
    "        STOP_CHARS = ['(', ')', '[', ']', '{', '}', ',', ':', ';', '.']\n",
    "\n",
    "        completions_w_probs = []\n",
    "        for choice in output['choices']:\n",
    "            completion = choice['text'].strip()\n",
    "            \n",
    "            if completion:\n",
    "                cleaned = self.clean_completion(completion, STOP_CHARS)\n",
    "                \n",
    "                if cleaned:\n",
    "                    # Simple: just get first token's probability\n",
    "                    try:\n",
    "                        first_logprob = choice['logprobs']['token_logprobs'][0]\n",
    "                        if first_logprob is not None:\n",
    "                            probability = math.exp(first_logprob)\n",
    "                        else:\n",
    "                            probability = 1.0\n",
    "                    except (KeyError, IndexError, TypeError):\n",
    "                        probability = 1.0  # Fallback\n",
    "                    \n",
    "                    \n",
    "                    completions_w_probs.append((cleaned, probability))\n",
    "        \n",
    "            # Remove duplicates, keep highest probability\n",
    "            seen = {}\n",
    "            for text, prob in completions_w_probs:\n",
    "                if text not in seen or prob > seen[text]:\n",
    "                    seen[text] = prob\n",
    "            \n",
    "            result = list(seen.items())\n",
    "            result.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8336f53",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b5b4c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tokenize import tokenize, ENCODING, ENDMARKER, COMMENT\n",
    "from io import BytesIO\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, basedir, infile, outfile):\n",
    "        self.basedir = basedir\n",
    "        self.infile = infile\n",
    "        self.outfile = outfile\n",
    "        self.tokens = []\n",
    "    \n",
    "    def tokenize_data(self):\n",
    "        file_paths = open(os.path.join(self.basedir, self.infile)).readlines()\n",
    "        files_with_tokens = []\n",
    "        for ct, path in enumerate(file_paths):\n",
    "            try:\n",
    "                # print(path)\n",
    "                code = open(os.path.join(self.basedir, path.strip())).read()\n",
    "                token_gen = tokenize(BytesIO(bytes(code, \"utf8\")).readline)\n",
    "                \n",
    "                file_tokens = []\n",
    "                for toknum, tokval, start, end, line in token_gen:\n",
    "                    tokval = \" \".join(tokval.split())\n",
    "                    \n",
    "                    if toknum in [ENCODING, ENDMARKER, COMMENT] or len(tokval) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    start_line, start_col = start\n",
    "                    \n",
    "                    # Convert (line, col) to character position\n",
    "                    lines = code.split('\\n')\n",
    "                    char_pos = sum(len(l) + 1 for l in lines[:start_line-1])\n",
    "                    char_pos += start_col\n",
    "                    \n",
    "                    file_tokens.append({\n",
    "                        'value': tokval,\n",
    "                        'start_pos': char_pos,\n",
    "                        'type': toknum\n",
    "                    })\n",
    "\n",
    "                    # print(f\"tokval: {tokval}, from_pos: {code[char_pos:char_pos+len(tokval)]}\")\n",
    "                \n",
    "                files_with_tokens.append({\n",
    "                    'file': path.strip(),\n",
    "                    'token_count': len(file_tokens),\n",
    "                    'tokens': file_tokens\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Tokenization error: {e}\")\n",
    "\n",
    "        return files_with_tokens\n",
    "\n",
    "    def get_data(self):\n",
    "        token_path = self.outfile\n",
    "        if os.path.exists(token_path):            \n",
    "            with open(token_path, 'r') as f:\n",
    "                files_with_tokens = json.load(f)\n",
    "\n",
    "            total_tokens = sum(f['token_count'] for f in files_with_tokens)\n",
    "            print(f\"Loaded {len(files_with_tokens)} files with {total_tokens} total tokens from {token_path}\")\n",
    "\n",
    "        else:            \n",
    "            files_with_tokens = self.tokenize_data()\n",
    "            \n",
    "            # Save to JSON\n",
    "            with open(token_path, 'w') as f:\n",
    "                json.dump(files_with_tokens, f, indent=2)\n",
    "\n",
    "            total_tokens = sum(f['token_count'] for f in files_with_tokens)\n",
    "            print(f\"Saved {len(files_with_tokens)} files with {total_tokens} total tokens to {token_path}\")\n",
    "\n",
    "        return files_with_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee44492",
   "metadata": {},
   "source": [
    "# Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6da152e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "TRIGGER_OPERATORS = {\n",
    "    '=', '==', '!=', '<', '>', '<=', '>=',\n",
    "    '+', '-', '*', '/', '//', '%', '**',\n",
    "    '+=', '-=', '*=', '/=',\n",
    "    'and', 'or', 'not', 'in', 'is',\n",
    "}\n",
    "\n",
    "TRIGGER_STRUCTURAL = {\n",
    "    '(', '[', '{',  # Opening brackets\n",
    "    ',',            # Comma (in function calls, lists, etc.)\n",
    "    '.',            # Dot (for attribute access)\n",
    "    ':',            # Colon (after if/for/def, or type hints)\n",
    "    '->',           # Type hint arrow\n",
    "}\n",
    "\n",
    "CLOSING_BRACES = {\n",
    "    ')', ']', '}',\n",
    "}\n",
    "\n",
    "VALID_TRIGGERS = TRIGGER_STRUCTURAL | TRIGGER_OPERATORS\n",
    "\n",
    "class SampleGenerator:\n",
    "    \"\"\"Generates prediction samples from tokenized files.\"\"\"\n",
    "    \n",
    "    def __init__(self, basedir, samples_file, \n",
    "                 n_before=10, n_after=10,\n",
    "                 valid_triggers=None, one_per_file=False, seed=42):\n",
    "        self.basedir = basedir\n",
    "        self.samples_file = samples_file\n",
    "        self.n_before = n_before\n",
    "        self.n_after = n_after\n",
    "        self.valid_triggers = valid_triggers if valid_triggers else VALID_TRIGGERS\n",
    "        self.one_per_file = one_per_file\n",
    "        self.seed = seed\n",
    "    \n",
    "    def create_samples(self, files_with_tokens):\n",
    "\n",
    "        # if self.one_per_file:\n",
    "        #     random.seed(self.seed)\n",
    "        \n",
    "        samples = []\n",
    "\n",
    "        for file_data in files_with_tokens:\n",
    "            file_path = file_data['file']\n",
    "            code = open(os.path.join(self.basedir, file_path.strip())).read()\n",
    "            tokens = file_data['tokens']\n",
    "            \n",
    "            # Skip files with too few tokens\n",
    "            min_tokens_needed = self.n_before + self.n_after + 1\n",
    "            if len(tokens) < min_tokens_needed:\n",
    "                continue\n",
    "            \n",
    "            # Find all valid indices where trigger token is valid\n",
    "            valid_token_idxs = []\n",
    "            for i in range(self.n_before, len(tokens) - self.n_after):\n",
    "                # The trigger is the token just before the target (at index i-1)\n",
    "                trigger_token = tokens[i - 1]['value']\n",
    "                target_token = tokens[i]['value']\n",
    "                \n",
    "                if ((trigger_token in self.valid_triggers) and \n",
    "                    (target_token not in self.valid_triggers | CLOSING_BRACES)):\n",
    "                    valid_token_idxs.append(i)\n",
    "            \n",
    "            # Skip files with no valid trigger positions\n",
    "            if len(valid_token_idxs) == 0:                \n",
    "                continue            \n",
    "            \n",
    "            # Select indices based on one_per_file setting\n",
    "            if self.one_per_file:\n",
    "                selected_token_idxs = [random.choice(valid_token_idxs)]\n",
    "            else:\n",
    "                selected_token_idxs = valid_token_idxs\n",
    "            \n",
    "            # Create samples for selected indices\n",
    "            for target_idx in selected_token_idxs:\n",
    "                \n",
    "                # Get context before\n",
    "                before_start = tokens[target_idx - self.n_before][\"start_pos\"]\n",
    "                before_end = tokens[target_idx][\"start_pos\"]\n",
    "                code_before = code[before_start:before_end]\n",
    "\n",
    "                # Get context after                \n",
    "                after_start = tokens[target_idx][\"start_pos\"] + len(tokens[target_idx][\"value\"])\n",
    "                after_end = tokens[target_idx + self.n_after][\"start_pos\"] + len(tokens[target_idx][\"value\"])\n",
    "                code_after = code[after_start:after_end]\n",
    "                \n",
    "                # Target token\n",
    "                target_token = tokens[target_idx]['value']\n",
    "                target_start_pos = tokens[target_idx]['start_pos']                \n",
    "                \n",
    "                # The trigger is the last token in code_before\n",
    "                trigger_token = tokens[target_idx-1]['value']\n",
    "                \n",
    "                samples.append({\n",
    "                    'file': file_path,\n",
    "                    'trigger_token': trigger_token,\n",
    "                    'target_token': target_token,\n",
    "                    'target_start_pos': target_start_pos,                                    \n",
    "                    'code_before': code_before,\n",
    "                    'code_after': code_after\n",
    "                })\n",
    "        \n",
    "        print(f\"Created {len(samples)} samples\")\n",
    "        if self.one_per_file:\n",
    "            print(f\"Mode: One sample per file (randomly selected)\")\n",
    "        else:\n",
    "            print(f\"Mode: All valid positions\")\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def get_samples(self, files_with_tokens):\n",
    "        \"\"\"Load or create prediction samples.\"\"\"\n",
    "        samples_path = self.samples_file\n",
    "        \n",
    "        if os.path.exists(samples_path):\n",
    "            print(f\"Loading samples from cache: {samples_path}\")\n",
    "            with open(samples_path, 'r') as f:\n",
    "                samples = json.load(f)\n",
    "            print(f\"Loaded {len(samples)} samples\")\n",
    "        else:\n",
    "            print(\"Creating samples...\")\n",
    "            samples = self.create_samples(files_with_tokens)\n",
    "            \n",
    "            # Save samples\n",
    "            print(f\"\\nSaving {len(samples)} samples to {samples_path}\")\n",
    "            with open(samples_path, 'w') as f:\n",
    "                json.dump(samples, f, indent=2)\n",
    "            print(\"Samples saved successfully!\")\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad548d5b",
   "metadata": {},
   "source": [
    "# Evaluator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionEvaluator:\n",
    "    def __init__(self, model: LocalCodeModel, lsp: LSPClient, basedir: str):\n",
    "        self.model = model\n",
    "        self.lsp = lsp\n",
    "        self.basedir = basedir\n",
    "    \n",
    "    def filter_completions_by_prob(self, completions, prob_threshold: float=0.8):\n",
    "        filtered = []\n",
    "        for completion, prob in completions:\n",
    "            if prob > prob_threshold:\n",
    "                filtered.append((completion, prob))\n",
    "        return filtered\n",
    "\n",
    "    def filter_completions_by_char(self, completions):\n",
    "        STOP_CHARS = ['(', ')', '[', ']', '{', '}', ',', ':', ';', '.', \"\\\"\", \"\\'\"]\n",
    "        filtered = []\n",
    "        for completion, prob in completions:\n",
    "            if completion not in STOP_CHARS:\n",
    "                filtered.append((completion, prob))\n",
    "        return filtered\n",
    "    \n",
    "    def filter_by_correctness(self, completions, code_before: str, code_after: str):\n",
    "        filtered = []\n",
    "        for completion, prob in completions:            \n",
    "            modified_code = code_before + completion + code_after                \n",
    "            is_valid = self.lsp.validate_code(modified_code)\n",
    "            \n",
    "            if is_valid:\n",
    "                filtered.append((completion, prob))\n",
    "                \n",
    "        return filtered\n",
    "        \n",
    "    def evaluate(self, samples, n: int, prob_threshold: float=0.8):\n",
    "\n",
    "        if len(samples) == 0:\n",
    "            raise ValueError(\"no samples provided\")\n",
    "        if len(samples) < n:\n",
    "            raise ValueError(f\"Not enough samples provided. Must provide at least {n}\")\n",
    "        \n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        i = 0\n",
    "\n",
    "        while n_total < n:\n",
    "            if i >= len(samples):\n",
    "                print(f\"Terminated early. Only evaluating over {n_total} samples\")\n",
    "                break\n",
    "\n",
    "            sample = samples[i]\n",
    "            code_before, code_after = sample['code_before'], sample['code_after']            \n",
    "            label = sample['target_token']\n",
    "            \n",
    "            print(f\"\\n--- Sample {i+1}, Valid Sample {n_total+1} ---\")\n",
    "            print(f\"Before: {repr(code_before[len(code_before)-20:])}\")\n",
    "            print(f\"After: {repr(code_after[:20])})\")                       \n",
    "            print(f\"Expected: {label}\")\n",
    "            \n",
    "            \n",
    "            start_time = time.perf_counter()\n",
    "            completions = self.model.generate(code_before=code_before, code_after=code_after)\n",
    "            end_time = time.perf_counter()\n",
    "            print(\"latency_ms:\", (end_time - start_time) * 1000)\n",
    "\n",
    "            print(f\"LLM completions: {completions}\")\n",
    "            \n",
    "            filter_start_time = time.perf_counter()\n",
    "            filtered = self.filter_completions_by_prob(completions=completions, prob_threshold=prob_threshold)\n",
    "            filtered = self.filter_completions_by_char(completions=filtered)\n",
    "            filtered = self.filter_by_correctness(completions=filtered, code_before=code_before, code_after=code_after)\n",
    "            filter_end_time = time.perf_counter()\n",
    "            print(\"filter latency_ms:\", (filter_end_time - filter_start_time) * 1000)\n",
    "\n",
    "            if not filtered:\n",
    "                print(f\"No completions above {prob_threshold:.1%} threshold. Skipping sample...\")\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            found_match = False\n",
    "            matching_prob = 0.0\n",
    "\n",
    "            for completion, prob in filtered:\n",
    "\n",
    "                # check filled code with LSP - might not be necessary\n",
    "                # modified_code = code_before + completion + code_after                \n",
    "                # is_valid = self.lsp.validate_code(modified_code)\n",
    "\n",
    "                if completion == label:\n",
    "                    found_match = True\n",
    "                    matching_prob = prob\n",
    "                    break\n",
    "            \n",
    "            if found_match:\n",
    "                print(f\"Correct! Pred:`{completion}` ({matching_prob:.1%})\")\n",
    "                n_correct += 1\n",
    "            else:\n",
    "                best_pred, best_prob = filtered[0]\n",
    "                print(f\"Incorrect! Pred: `{best_pred}` ({best_prob:.1%}) Label: `{label}`\")\n",
    "            \n",
    "            n_total += 1\n",
    "            i += 1\n",
    "        \n",
    "        acc = n_correct / n_total\n",
    "        print(f\"\\nAccuracy over {n_total} samples: {acc:.1%} (Coverage: {(n_total/i):.1%}, Prob TH: {prob_threshold:.1%})\")\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91b17b",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eb410799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 98 files with 85376 total tokens from ./data/eval_tokens.json\n",
      "Loading samples from cache: ./data/samples.json\n",
      "Loaded 24741 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token budget: 1998 tokens (~4995 chars) available for input\n",
      "\n",
      "--- Sample 1, Valid Sample 1 ---\n",
      "Before: ' m[\"out\"][\"format\"].'\n",
      "After: '() )\\n\\t\\tself.assertEq')\n",
      "Expected: hash\n",
      "latency_ms: 599.5839997194707\n",
      "LLM completions: [('hash', 0.9789388207753184)]\n",
      "filter latency_ms: 510.13804180547595\n",
      "Correct! Pred:`hash` (97.9%)\n",
      "\n",
      "--- Sample 2, Valid Sample 2 ---\n",
      "Before: ' elif hasattr(self, '\n",
      "After: '):\\n            genre')\n",
      "Expected: \"arxiv\"\n",
      "latency_ms: 1620.0309582054615\n",
      "LLM completions: [('\"arxiv_id\"', 0.9338810453067274)]\n",
      "filter latency_ms: 506.33891578763723\n",
      "Incorrect! Pred: `\"arxiv_id\"` (93.4%) Label: `\"arxiv\"`\n",
      "\n",
      "--- Sample 3, Valid Sample 3 ---\n",
      "Before: '_,\\n            self.'\n",
      "After: '()\\n        )\\n\\n\\nclass')\n",
      "Expected: to_header\n",
      "latency_ms: 1314.2638746649027\n",
      "LLM completions: [('to_header', 0.9200420954028997)]\n",
      "filter latency_ms: 506.62395916879177\n",
      "Correct! Pred:`to_header` (92.0%)\n",
      "\n",
      "--- Sample 4, Valid Sample 4 ---\n",
      "Before: '.__parent.ancestor( '\n",
      "After: '.ScriptNode ) ) :\\n\\t\\t')\n",
      "Expected: Gaffer\n",
      "latency_ms: 910.3692499920726\n",
      "LLM completions: [('Gaffer', 0.828203636418069)]\n",
      "filter latency_ms: 506.21637515723705\n",
      "Correct! Pred:`Gaffer` (82.8%)\n",
      "\n",
      "--- Sample 5, Valid Sample 5 ---\n",
      "Before: 'raise self.KeyError('\n",
      "After: '(e))\\n        for buc')\n",
      "Expected: str\n",
      "latency_ms: 974.1441672667861\n",
      "LLM completions: [('str', 0.9989892932805439)]\n",
      "filter latency_ms: 506.3967499881983\n",
      "Correct! Pred:`str` (99.9%)\n",
      "\n",
      "--- Sample 6, Valid Sample 6 ---\n",
      "Before: '    def accept_html('\n",
      "After: '):\\n        \"\"\"True i')\n",
      "Expected: self\n",
      "latency_ms: 822.7140828967094\n",
      "LLM completions: [('self', 0.604425572495737)]\n",
      "filter latency_ms: 0.005000270903110504\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 7, Valid Sample 6 ---\n",
      "Before: 'er = random.randint('\n",
      "After: ', total_instances-1)')\n",
      "Expected: 0\n",
      "latency_ms: 1013.4464157745242\n",
      "LLM completions: [('0', 0.9872910205067219)]\n",
      "filter latency_ms: 510.82966569811106\n",
      "Correct! Pred:`0` (98.7%)\n",
      "\n",
      "--- Sample 8, Valid Sample 7 ---\n",
      "Before: 'f.__metadataWidgets.'\n",
      "After: '() :\\n\\t\\t\\twidget.setTa')\n",
      "Expected: values\n",
      "latency_ms: 784.4218341633677\n",
      "LLM completions: [('values', 0.9358072517897873)]\n",
      "filter latency_ms: 509.0202088467777\n",
      "Correct! Pred:`values` (93.6%)\n",
      "\n",
      "--- Sample 9, Valid Sample 8 ---\n",
      "Before: 'qop = _set_property('\n",
      "After: \", doc='''\\n        A \")\n",
      "Expected: 'qop'\n",
      "latency_ms: 2073.6579997465014\n",
      "LLM completions: [(\"'qop'\", 0.9463666279287172)]\n",
      "filter latency_ms: 507.93912494555116\n",
      "Correct! Pred:`'qop'` (94.6%)\n",
      "\n",
      "--- Sample 10, Valid Sample 9 ---\n",
      "Before: ',\\n\\t\\t\\t\\t\\t\"checkBox\" : '\n",
      "After: ' == g.metadata,\\n\\t\\t\\t\\t')\n",
      "Expected: metadata\n",
      "latency_ms: 992.1492501161993\n",
      "LLM completions: [('Gaffer', 0.6632016934497517)]\n",
      "filter latency_ms: 0.004165805876255035\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 11, Valid Sample 9 ---\n",
      "Before: '            if self.'\n",
      "After: '(value, item):\\n     ')\n",
      "Expected: _value_matches\n",
      "latency_ms: 1094.5112495683134\n",
      "LLM completions: [('_value_matches', 0.9993481120297208)]\n",
      "filter latency_ms: 506.44604163244367\n",
      "Correct! Pred:`_value_matches` (99.9%)\n",
      "\n",
      "--- Sample 12, Valid Sample 10 ---\n",
      "Before: '                    '\n",
      "After: ' = arg.replace(tzinf')\n",
      "Expected: arg\n",
      "latency_ms: 1209.6473332494497\n",
      "LLM completions: [('tzs', 0.6351686887210578)]\n",
      "filter latency_ms: 0.0033332034945487976\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 13, Valid Sample 10 ---\n",
      "Before: 'key\"\\n\\nBOARDS_FILE = '\n",
      "After: \" + '.BOARDS'\\nSTRLEN \")\n",
      "Expected: BBS_ROOT\n",
      "latency_ms: 1365.3621249832213\n",
      "LLM completions: [('BBS_ROOT', 0.9504688449985143)]\n",
      "filter latency_ms: 508.2755000330508\n",
      "Correct! Pred:`BBS_ROOT` (95.0%)\n",
      "\n",
      "--- Sample 14, Valid Sample 11 ---\n",
      "Before: '        for item in '\n",
      "After: ':\\n            yield ')\n",
      "Expected: self\n",
      "latency_ms: 1086.6947500035167\n",
      "LLM completions: [('self', 0.9864635676192807)]\n",
      "filter latency_ms: 506.82987505570054\n",
      "Correct! Pred:`self` (98.6%)\n",
      "\n",
      "--- Sample 15, Valid Sample 12 ---\n",
      "Before: \"urce_id'] = request.\"\n",
      "After: \"('resource_id')\\n    \")\n",
      "Expected: get\n",
      "latency_ms: 638.0713325925171\n",
      "LLM completions: [('get', 0.9995655517431101)]\n",
      "filter latency_ms: 507.86300003528595\n",
      "Correct! Pred:`get` (100.0%)\n",
      "\n",
      "--- Sample 16, Valid Sample 13 ---\n",
      "Before: '\\n            result.'\n",
      "After: '(value)\\n        retu')\n",
      "Expected: append\n",
      "latency_ms: 1087.2738752514124\n",
      "LLM completions: [('append', 0.9971429906549948)]\n",
      "filter latency_ms: 508.0917081795633\n",
      "Correct! Pred:`append` (99.7%)\n",
      "\n",
      "--- Sample 17, Valid Sample 14 ---\n",
      "Before: '                    '\n",
      "After: '._classes_registry.a')\n",
      "Expected: self\n",
      "latency_ms: 1031.7553747445345\n",
      "LLM completions: [('self', 0.9860224869941322)]\n",
      "filter latency_ms: 508.53558303788304\n",
      "Correct! Pred:`self` (98.6%)\n",
      "\n",
      "--- Sample 18, Valid Sample 15 ---\n",
      "Before: '      return codecs.'\n",
      "After: '(name).name\\n        ')\n",
      "Expected: lookup\n",
      "latency_ms: 1210.19854163751\n",
      "LLM completions: [('getencoder', 0.46592231385216265)]\n",
      "filter latency_ms: 0.009542331099510193\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 19, Valid Sample 15 ---\n",
      "Before: '               self.'\n",
      "After: '.append(registry_pat')\n",
      "Expected: _bios_registry_locations\n",
      "latency_ms: 1705.4313328117132\n",
      "LLM completions: [('_bios_registry_locations', 0.9650438540817388)]\n",
      "filter latency_ms: 512.02258374542\n",
      "Correct! Pred:`_bios_registry_locations` (96.5%)\n",
      "\n",
      "--- Sample 20, Valid Sample 16 ---\n",
      "Before: '          jsonreg = '\n",
      "After: '.loads(location_file')\n",
      "Expected: json\n",
      "latency_ms: 764.614500105381\n",
      "LLM completions: [('json', 0.966545746962399)]\n",
      "filter latency_ms: 508.0717080272734\n",
      "Correct! Pred:`json` (96.7%)\n",
      "\n",
      "--- Sample 21, Valid Sample 17 ---\n",
      "Before: 'n self:\\n            '\n",
      "After: ' len(newval) > int(s')\n",
      "Expected: if\n",
      "latency_ms: 965.5595826916397\n",
      "LLM completions: [('if', 0.9985204209376067)]\n",
      "filter latency_ms: 506.7024170421064\n",
      "Correct! Pred:`if` (99.9%)\n",
      "\n",
      "--- Sample 22, Valid Sample 18 ---\n",
      "Before: 'if (stat.st_mtime > '\n",
      "After: '.update_time):\\n     ')\n",
      "Expected: self\n",
      "latency_ms: 784.3696670606732\n",
      "LLM completions: [('self', 0.9687814674088575)]\n",
      "filter latency_ms: 507.58779188618064\n",
      "Correct! Pred:`self` (96.9%)\n",
      "\n",
      "--- Sample 23, Valid Sample 19 ---\n",
      "Before: 'stance(ret_val, api.'\n",
      "After: '.VPNService)\\n\\n    @t')\n",
      "Expected: vpn\n",
      "latency_ms: 929.8662496730685\n",
      "LLM completions: [('vpn', 0.24992949669620473)]\n",
      "filter latency_ms: 0.0069998204708099365\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 24, Valid Sample 19 ---\n",
      "Before: 'lf.__target, Gaffer.'\n",
      "After: ' ) :\\n\\t\\t\\tself.__metad')\n",
      "Expected: Node\n",
      "latency_ms: 703.3660840243101\n",
      "LLM completions: [('Node', 0.9985634488905634)]\n",
      "filter latency_ms: 507.17045832425356\n",
      "Correct! Pred:`Node` (99.9%)\n",
      "\n",
      "--- Sample 25, Valid Sample 20 ---\n",
      "Before: \"u'Type'].lower() == \"\n",
      "After: ':\\n                re')\n",
      "Expected: u'integer'\n",
      "latency_ms: 1315.1118326932192\n",
      "LLM completions: [(\"u'integer'\", 0.8818171136187225)]\n",
      "filter latency_ms: 507.54424976184964\n",
      "Correct! Pred:`u'integer'` (88.2%)\n",
      "\n",
      "--- Sample 26, Valid Sample 21 ---\n",
      "Before: 'on=True, registries='\n",
      "After: ', datareturn=True))\\n')\n",
      "Expected: True\n",
      "latency_ms: 1005.6190839968622\n",
      "LLM completions: [('True', 0.314082517777952)]\n",
      "filter latency_ms: 0.003875233232975006\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 27, Valid Sample 21 ---\n",
      "Before: '\\n            if len('\n",
      "After: \") > int(self[u'MaxLe\")\n",
      "Expected: newval\n",
      "latency_ms: 966.8285828083754\n",
      "LLM completions: [('newval', 0.9991865819672049)]\n",
      "filter latency_ms: 506.3529168255627\n",
      "Correct! Pred:`newval` (99.9%)\n",
      "\n",
      "--- Sample 28, Valid Sample 22 ---\n",
      "Before: 'UI.Widget.__init__( '\n",
      "After: ', scrolledContainer,')\n",
      "Expected: self\n",
      "latency_ms: 898.6421669833362\n",
      "LLM completions: [('self', 0.9553236362022479)]\n",
      "filter latency_ms: 509.71508398652077\n",
      "Correct! Pred:`self` (95.5%)\n",
      "\n",
      "--- Sample 29, Valid Sample 23 ---\n",
      "Before: 'end\\n\\n    def extend('\n",
      "After: ', iterable):\\n       ')\n",
      "Expected: self\n",
      "latency_ms: 784.2077086679637\n",
      "LLM completions: [('self', 0.9987785257761258)]\n",
      "filter latency_ms: 511.0040004365146\n",
      "Correct! Pred:`self` (99.9%)\n",
      "\n",
      "--- Sample 30, Valid Sample 24 ---\n",
      "Before: 'or self.mtitle.find('\n",
      "After: ') != -1):\\n          ')\n",
      "Expected: \"(BM: SYSOPS)\"\n",
      "latency_ms: 645.1014582999051\n",
      "LLM completions: [('\"', 0.929435084265599)]\n",
      "filter latency_ms: 0.0029159709811210632\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 31, Valid Sample 24 ---\n",
      "Before: '4 + [0.99 * 0.99] * '\n",
      "After: '\\n\\n    values = [sch(')\n",
      "Expected: 4\n",
      "latency_ms: 765.1036661118269\n",
      "LLM completions: [('4', 0.7295522808604512)]\n",
      "filter latency_ms: 0.0027921050786972046\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 32, Valid Sample 24 ---\n",
      "Before: 'ls, predicted, mask='\n",
      "After: '):\\n        errors = ')\n",
      "Expected: None\n",
      "latency_ms: 615.9105002880096\n",
      "LLM completions: [('None', 0.9951627537374809)]\n",
      "filter latency_ms: 510.04416728392243\n",
      "Correct! Pred:`None` (99.5%)\n",
      "\n",
      "--- Sample 33, Valid Sample 25 ---\n",
      "Before: '_list2 = [alias_row.'\n",
      "After: ' for alias_row in pr')\n",
      "Expected: my_alias_tuple_for_comparing\n",
      "latency_ms: 1536.9131248444319\n",
      "LLM completions: [('my_alias_tuple_for_comparing', 0.9992720423155108)]\n",
      "filter latency_ms: 507.94637529179454\n",
      "Correct! Pred:`my_alias_tuple_for_comparing` (99.9%)\n",
      "\n",
      "--- Sample 34, Valid Sample 26 ---\n",
      "Before: '\\n            result.'\n",
      "After: '(SchemaValidationErr')\n",
      "Expected: append\n",
      "latency_ms: 895.9764167666435\n",
      "LLM completions: [('append', 0.9994180015480927)]\n",
      "filter latency_ms: 514.5615832880139\n",
      "Correct! Pred:`append` (99.9%)\n",
      "\n",
      "--- Sample 35, Valid Sample 27 ---\n",
      "Before: '\\n            task = '\n",
      "After: '._activity.wait4task')\n",
      "Expected: self\n",
      "latency_ms: 680.3432912565768\n",
      "LLM completions: [('self', 0.9993662085013781)]\n",
      "filter latency_ms: 508.8738747872412\n",
      "Correct! Pred:`self` (99.9%)\n",
      "\n",
      "--- Sample 36, Valid Sample 28 ---\n",
      "Before: 'nnotLockAllFeatures('\n",
      "After: '.OWSException):\\n    ')\n",
      "Expected: common\n",
      "latency_ms: 1062.0220419950783\n",
      "LLM completions: [('common', 0.994036568538593)]\n",
      "filter latency_ms: 508.95087514072657\n",
      "Correct! Pred:`common` (99.4%)\n",
      "\n",
      "--- Sample 37, Valid Sample 29 ---\n",
      "Before: '\", \\n                '\n",
      "After: ': \"provider_data_dum')\n",
      "Expected: \"type\"\n",
      "latency_ms: 1884.630875196308\n",
      "LLM completions: [('\"type\"', 0.9508021609246418)]\n",
      "filter latency_ms: 508.0849160440266\n",
      "Correct! Pred:`\"type\"` (95.1%)\n",
      "\n",
      "--- Sample 38, Valid Sample 30 ---\n",
      "Before: 'yError:\\n            '\n",
      "After: ' []\\n\\n\\n\\n    @cached_p')\n",
      "Expected: return\n",
      "latency_ms: 1116.3617088459432\n",
      "LLM completions: [('return', 0.976907274390021)]\n",
      "filter latency_ms: 509.7140418365598\n",
      "Correct! Pred:`return` (97.7%)\n",
      "\n",
      "--- Sample 39, Valid Sample 31 ---\n",
      "Before: 'for tweet_subset in '\n",
      "After: ':\\n        tweet_id_s')\n",
      "Expected: list_of_groups\n",
      "latency_ms: 1282.349542248994\n",
      "LLM completions: [('list_of_groups', 0.9886166477643745)]\n",
      "filter latency_ms: 510.61254227533937\n",
      "Correct! Pred:`list_of_groups` (98.9%)\n",
      "\n",
      "--- Sample 40, Valid Sample 32 ---\n",
      "Before: 'sertEqual( i[\"out\"]['\n",
      "After: '].getValue(), m[\"out')\n",
      "Expected: \"dataWindow\"\n",
      "latency_ms: 749.2691245861351\n",
      "LLM completions: [('\"dataWindow\"', 0.9421203974950255)]\n",
      "filter latency_ms: 514.4037920981646\n",
      "Correct! Pred:`\"dataWindow\"` (94.2%)\n",
      "\n",
      "--- Sample 41, Valid Sample 33 ---\n",
      "Before: 'atText.setText(self.'\n",
      "After: '[\"Format\"])\\n        ')\n",
      "Expected: data\n",
      "latency_ms: 571.3952500373125\n",
      "LLM completions: [('data', 0.999541969367451)]\n",
      "filter latency_ms: 511.6184172220528\n",
      "Correct! Pred:`data` (100.0%)\n",
      "\n",
      "--- Sample 42, Valid Sample 34 ---\n",
      "Before: 'gadgetMenu.setText( '\n",
      "After: '.label )\\n\\t\\t\\t\\treturn\\n')\n",
      "Expected: g\n",
      "latency_ms: 753.8536670617759\n",
      "LLM completions: [('g', 0.8486854361625404)]\n",
      "filter latency_ms: 507.61495903134346\n",
      "Correct! Pred:`g` (84.9%)\n",
      "\n",
      "--- Sample 43, Valid Sample 35 ---\n",
      "Before: 'rvice(\\n            {'\n",
      "After: ': form_data}).AndRet')\n",
      "Expected: 'vpnservice'\n",
      "latency_ms: 1364.3067092634737\n",
      "LLM completions: [(\"'vpnservice'\", 0.902525100714104)]\n",
      "filter latency_ms: 512.0799168944359\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 44, Valid Sample 35 ---\n",
      "Before: 'f items(self, multi='\n",
      "After: '):\\n        \"\"\"Return')\n",
      "Expected: False\n",
      "latency_ms: 1666.5368750691414\n",
      "LLM completions: [('False', 0.9853447060823181)]\n",
      "filter latency_ms: 512.4807497486472\n",
      "Correct! Pred:`False` (98.5%)\n",
      "\n",
      "--- Sample 45, Valid Sample 36 ---\n",
      "Before: 'f, header):\\n        '\n",
      "After: '       :param header')\n",
      "Expected: \"\"\"Remove a header from the set. This raises an :exc:`KeyError` if the header is not in the set. .. versionchanged:: 0.5 In older versions a :exc:`IndexError` was raised instead of a :exc:`KeyError` if the object was missing. :param header: the header to be removed. \"\"\"\n",
      "latency_ms: 1258.336375001818\n",
      "LLM completions: [('\"\"\"Remove', 0.9069117362947561)]\n",
      "filter latency_ms: 509.53566562384367\n",
      "Incorrect! Pred: `\"\"\"Remove` (90.7%) Label: `\"\"\"Remove a header from the set. This raises an :exc:`KeyError` if the header is not in the set. .. versionchanged:: 0.5 In older versions a :exc:`IndexError` was raised instead of a :exc:`KeyError` if the object was missing. :param header: the header to be removed. \"\"\"`\n",
      "\n",
      "--- Sample 46, Valid Sample 37 ---\n",
      "Before: 'tion\\n\\n\\t\\twith Gaffer.'\n",
      "After: '( self.__plugParent.')\n",
      "Expected: UndoContext\n",
      "latency_ms: 866.43704213202\n",
      "LLM completions: [('Context', 0.3449837448653845)]\n",
      "filter latency_ms: 0.002709217369556427\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 47, Valid Sample 37 ---\n",
      "Before: 'ericSlider( value = '\n",
      "After: ', min = 0, max = 2 )')\n",
      "Expected: 0.1\n",
      "latency_ms: 818.1632086634636\n",
      "LLM completions: [('2', 0.449996998540551)]\n",
      "filter latency_ms: 0.0031669624149799347\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 48, Valid Sample 37 ---\n",
      "Before: 'ionPath.rpartition( '\n",
      "After: ' )\\n\\t\\t\\t\\tparentSection')\n",
      "Expected: \".\"\n",
      "latency_ms: 717.3740002326667\n",
      "LLM completions: [('\"', 0.6700204725668967)]\n",
      "filter latency_ms: 0.002750195562839508\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 49, Valid Sample 37 ---\n",
      "Before: \"sk and task['type'].\"\n",
      "After: \"('Task'):\\n          \")\n",
      "Expected: startswith\n",
      "latency_ms: 763.8171669095755\n",
      "LLM completions: [('lower', 0.5425193015766724)]\n",
      "filter latency_ms: 0.0025420449674129486\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 50, Valid Sample 37 ---\n",
      "Before: \"er:              ', \"\n",
      "After: \"['boot']['order'], '\")\n",
      "Expected: profile_template\n",
      "latency_ms: 953.9398332126439\n",
      "LLM completions: [('profile_template', 0.9429078545767255)]\n",
      "filter latency_ms: 504.97320806607604\n",
      "Correct! Pred:`profile_template` (94.3%)\n",
      "\n",
      "--- Sample 51, Valid Sample 38 ---\n",
      "Before: ',\\n\\t\\t\\t\\t\\t\\t\\t_metadata( '\n",
      "After: '.getPlugParent(), na')\n",
      "Expected: self\n",
      "latency_ms: 940.4773754067719\n",
      "LLM completions: [('self', 0.8086192428020762)]\n",
      "filter latency_ms: 507.40425009280443\n",
      "Correct! Pred:`self` (80.9%)\n",
      "\n",
      "--- Sample 52, Valid Sample 39 ---\n",
      "Before: \"          'id':data.\"\n",
      "After: \"('id'),\\n          'r\")\n",
      "Expected: get\n",
      "latency_ms: 660.0830419920385\n",
      "LLM completions: [('get', 0.9998469586985235)]\n",
      "filter latency_ms: 508.1908330321312\n",
      "Correct! Pred:`get` (100.0%)\n",
      "\n",
      "--- Sample 53, Valid Sample 40 ---\n",
      "Before: ' setlist(self, key, '\n",
      "After: '):\\n        is_immuta')\n",
      "Expected: new_list\n",
      "latency_ms: 973.079124931246\n",
      "LLM completions: [('value_list', 0.5214315095588504)]\n",
      "filter latency_ms: 0.003041233867406845\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 54, Valid Sample 40 ---\n",
      "Before: 'rint(e)\\n\\n\\ndef login('\n",
      "After: ', credential):\\n    #')\n",
      "Expected: con\n",
      "latency_ms: 834.8454996012151\n",
      "LLM completions: [('srv', 0.7381573139795242)]\n",
      "filter latency_ms: 0.0027921050786972046\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 55, Valid Sample 40 ---\n",
      "Before: 'alues = [sch(epoch, '\n",
      "After: \", 'update', 3, None,\")\n",
      "Expected: update\n",
      "latency_ms: 854.1267500258982\n",
      "LLM completions: [('update', 0.9593006085844671)]\n",
      "filter latency_ms: 509.14433412253857\n",
      "Correct! Pred:`update` (95.9%)\n",
      "\n",
      "--- Sample 56, Valid Sample 41 ---\n",
      "Before: '_id.in_(tweet_ids)).'\n",
      "After: '():\\n        # logger')\n",
      "Expected: all\n",
      "latency_ms: 983.6746668443084\n",
      "LLM completions: [('filter', 0.48153846873257955)]\n",
      "filter latency_ms: 0.0038747675716876984\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 57, Valid Sample 41 ---\n",
      "Before: 'ait4task(task, tout='\n",
      "After: ', verbose=verbose)\\n ')\n",
      "Expected: 600\n",
      "latency_ms: 912.349124904722\n",
      "LLM completions: [('None', 0.25920737485763184)]\n",
      "filter latency_ms: 0.0025830231606960297\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 58, Valid Sample 41 ---\n",
      "Before: '                int('\n",
      "After: \"[u'MinLength'])), re\")\n",
      "Expected: self\n",
      "latency_ms: 812.2088331729174\n",
      "LLM completions: [('self', 0.9995393491895492)]\n",
      "filter latency_ms: 508.51058261469007\n",
      "Correct! Pred:`self` (100.0%)\n",
      "\n",
      "--- Sample 59, Valid Sample 42 ---\n",
      "Before: 'host_from_aliases()['\n",
      "After: ']\\n\\n    def get_host(')\n",
      "Expected: 0\n",
      "latency_ms: 1005.6611667387187\n",
      "LLM completions: [('0', 0.9990747199483151)]\n",
      "filter latency_ms: 510.31379168853164\n",
      "Correct! Pred:`0` (99.9%)\n",
      "\n",
      "--- Sample 60, Valid Sample 43 ---\n",
      "Before: \"t_format'].endswith(\"\n",
      "After: \"):\\n            if 'c\")\n",
      "Expected: 'json'\n",
      "latency_ms: 1119.0415830351412\n",
      "LLM completions: [(\"'json'\", 0.5588203805391798)]\n",
      "filter latency_ms: 0.0027921050786972046\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 61, Valid Sample 43 ---\n",
      "Before: 'lf, other):\\n        '\n",
      "After: ' not isinstance(othe')\n",
      "Expected: if\n",
      "latency_ms: 1159.2047498561442\n",
      "LLM completions: [('if', 0.7936057490847122)]\n",
      "filter latency_ms: 0.003125052899122238\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 62, Valid Sample 43 ---\n",
      "Before: ' )\\n\\n\\t\\tdef hasLabel( '\n",
      "After: ' ) :\\n\\n\\t\\t\\treturn True')\n",
      "Expected: self\n",
      "latency_ms: 1044.7052079252899\n",
      "LLM completions: [('self', 0.901621211144136)]\n",
      "filter latency_ms: 510.8267921023071\n",
      "Correct! Pred:`self` (90.2%)\n",
      "\n",
      "--- Sample 63, Valid Sample 44 ---\n",
      "Before: 'ChangedConnection = '\n",
      "After: '.Metadata.plugValueC')\n",
      "Expected: Gaffer\n",
      "latency_ms: 921.0498752072453\n",
      "LLM completions: [('plug', 0.5895910289220849)]\n",
      "filter latency_ms: 0.002874992787837982\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 64, Valid Sample 44 ---\n",
      "Before: ' path is None :\\n\\t\\t\\t\\t'\n",
      "After: '.__pathListing.setSe')\n",
      "Expected: self\n",
      "latency_ms: 618.6112090945244\n",
      "LLM completions: [('self', 0.9218512896124746)]\n",
      "filter latency_ms: 514.3865831196308\n",
      "Correct! Pred:`self` (92.2%)\n",
      "\n",
      "--- Sample 65, Valid Sample 45 ---\n",
      "Before: '  def update_server('\n",
      "After: ', server):\\n        t')\n",
      "Expected: self\n",
      "latency_ms: 703.5764162428677\n",
      "LLM completions: [('self', 0.9983176924392114)]\n",
      "filter latency_ms: 509.9517498165369\n",
      "Correct! Pred:`self` (99.8%)\n",
      "\n",
      "--- Sample 66, Valid Sample 46 ---\n",
      "Before: '    self.assertTrue('\n",
      "After: '.name, d.name)\\n     ')\n",
      "Expected: v\n",
      "latency_ms: 876.4752498827875\n",
      "LLM completions: [('v', 0.8918904658170784)]\n",
      "filter latency_ms: 507.4902502819896\n",
      "Correct! Pred:`v` (89.2%)\n",
      "\n",
      "--- Sample 67, Valid Sample 47 ---\n",
      "Before: '          out.write('\n",
      "After: ')\\n\\n        out.write')\n",
      "Expected: '\\n'\n",
      "latency_ms: 1023.1857500039041\n",
      "LLM completions: [(\"'\\\\n'\", 0.9592333651317828)]\n",
      "filter latency_ms: 508.59629176557064\n",
      "Correct! Pred:`'\\n'` (95.9%)\n",
      "\n",
      "--- Sample 68, Valid Sample 48 ---\n",
      "Before: '\\n\\t\\treturn \".\".join( '\n",
      "After: '.CamelCase.toSpaced(')\n",
      "Expected: IECore\n",
      "latency_ms: 1753.6061252467334\n",
      "LLM completions: [('GraphComponentNameFormatter', 0.21545718805710412)]\n",
      "filter latency_ms: 0.0029578804969787598\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 69, Valid Sample 48 ---\n",
      "Before: '       description, '\n",
      "After: ',\\n                  ')\n",
      "Expected: firmwareSettingsV3\n",
      "latency_ms: 2365.4634580016136\n",
      "LLM completions: [('firmwareSettingsV3', 0.9650430775472536)]\n",
      "filter latency_ms: 509.4223329797387\n",
      "Correct! Pred:`firmwareSettingsV3` (96.5%)\n",
      "\n",
      "--- Sample 70, Valid Sample 49 ---\n",
      "Before: ') )\\n\\n\\tdef testHash( '\n",
      "After: ' ) :\\n\\n\\t\\tc = Gaffer.C')\n",
      "Expected: self\n",
      "latency_ms: 844.2118330858648\n",
      "LLM completions: [('self', 0.9857037489411183)]\n",
      "filter latency_ms: 510.0511251948774\n",
      "Correct! Pred:`self` (98.6%)\n",
      "\n",
      "--- Sample 71, Valid Sample 50 ---\n",
      "Before: '(self): return self.'\n",
      "After: '.minute\\n    @propert')\n",
      "Expected: dt\n",
      "latency_ms: 1096.9031667336822\n",
      "LLM completions: [('dt', 0.9987151463773298)]\n",
      "filter latency_ms: 510.2852499112487\n",
      "Correct! Pred:`dt` (99.9%)\n",
      "\n",
      "--- Sample 72, Valid Sample 51 ---\n",
      "Before: ' pattern in regcopy.'\n",
      "After: '():\\n                ')\n",
      "Expected: iterkeys\n",
      "latency_ms: 816.2166671827435\n",
      "LLM completions: [('iterkeys', 0.8760941856721552)]\n",
      "filter latency_ms: 511.071499902755\n",
      "Correct! Pred:`iterkeys` (87.6%)\n",
      "\n",
      "--- Sample 73, Valid Sample 52 ---\n",
      "Before: '                    '\n",
      "After: ',\\n                  ')\n",
      "Expected: hideUnusedFlexNics\n",
      "latency_ms: 2419.7717076167464\n",
      "LLM completions: [('hideUnusedFlexNics', 0.9767952260830066)]\n",
      "filter latency_ms: 509.28120827302337\n",
      "Correct! Pred:`hideUnusedFlexNics` (97.7%)\n",
      "\n",
      "--- Sample 74, Valid Sample 53 ---\n",
      "Before: '= json.loads(result.'\n",
      "After: '())\\n                ')\n",
      "Expected: read\n",
      "latency_ms: 1026.5526250004768\n",
      "LLM completions: [('read', 0.9921980653030683)]\n",
      "filter latency_ms: 508.9335828088224\n",
      "Correct! Pred:`read` (99.2%)\n",
      "\n",
      "--- Sample 75, Valid Sample 54 ---\n",
      "Before: '                    '\n",
      "After: \" = True # but we'll \")\n",
      "Expected: avoid_localize\n",
      "latency_ms: 1392.655790783465\n",
      "LLM completions: [('avoid_local', 0.2970806364894174)]\n",
      "filter latency_ms: 0.004709232598543167\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 76, Valid Sample 54 ---\n",
      "Before: 'evel...\\n            '\n",
      "After: ' item\\n        else:\\n')\n",
      "Expected: return\n",
      "latency_ms: 924.5607499033213\n",
      "LLM completions: [('if', 0.5084207929061579)]\n",
      "filter latency_ms: 0.005166977643966675\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 77, Valid Sample 54 ---\n",
      "Before: '\\n\\n        for _, _, '\n",
      "After: ' in os.walk(schemadi')\n",
      "Expected: filenames\n",
      "latency_ms: 1307.6977082528174\n",
      "LLM completions: [('filenames', 0.9868806617874097)]\n",
      "filter latency_ms: 504.47450019419193\n",
      "Correct! Pred:`filenames` (98.7%)\n",
      "\n",
      "--- Sample 78, Valid Sample 55 ---\n",
      "Before: 'thListing.getPath().'\n",
      "After: '() ]\\n\\n\\t\\tname = \"New ')\n",
      "Expected: children\n",
      "latency_ms: 853.2319581136107\n",
      "LLM completions: [('getPaths', 0.5632237744612814)]\n",
      "filter latency_ms: 0.003375113010406494\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 79, Valid Sample 55 ---\n",
      "Before: 'FrameList ):\\n\\t\\t\\tdst.'\n",
      "After: ' = src.frameList\\n\\n\\t\\t')\n",
      "Expected: frameList\n",
      "latency_ms: 856.4303750172257\n",
      "LLM completions: [('frameList', 0.9888844619255003)]\n",
      "filter latency_ms: 506.75979210063815\n",
      "Correct! Pred:`frameList` (98.9%)\n",
      "\n",
      "--- Sample 80, Valid Sample 56 ---\n",
      "Before: '     _h.mult_add_st('\n",
      "After: ' - self.decay, mu_b,')\n",
      "Expected: 1.0\n",
      "latency_ms: 1051.8777500838041\n",
      "LLM completions: [('1', 0.916229310583953)]\n",
      "filter latency_ms: 512.8078749403358\n",
      "Incorrect! Pred: `1` (91.6%) Label: `1.0`\n",
      "\n",
      "--- Sample 81, Valid Sample 57 ---\n",
      "Before: 'm in self[attrname]['\n",
      "After: ']:\\n                v')\n",
      "Expected: 'oneOf'\n",
      "latency_ms: 1117.8625840693712\n",
      "LLM completions: [(\"'oneOf'\", 0.54759495267237)]\n",
      "filter latency_ms: 0.003334134817123413\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 82, Valid Sample 57 ---\n",
      "Before: ')\\n\\n\\t\\ts.setPosition( '\n",
      "After: ' )\\n\\t\\tself.assertEqua')\n",
      "Expected: 2\n",
      "latency_ms: 685.6806250289083\n",
      "LLM completions: []\n",
      "filter latency_ms: 0.0028330832719802856\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 83, Valid Sample 57 ---\n",
      "Before: 'se:\\n                '\n",
      "After: '\\n\\n            result')\n",
      "Expected: pass\n",
      "latency_ms: 1532.9995001666248\n",
      "LLM completions: [('return', 0.6155758566133522)]\n",
      "filter latency_ms: 0.0030831433832645416\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 84, Valid Sample 57 ---\n",
      "Before: '   if opaque is not '\n",
      "After: \":\\n            d['opa\")\n",
      "Expected: None\n",
      "latency_ms: 800.7049579173326\n",
      "LLM completions: [('None', 0.9985730773008933)]\n",
      "filter latency_ms: 507.0253750309348\n",
      "Correct! Pred:`None` (99.9%)\n",
      "\n",
      "--- Sample 85, Valid Sample 58 ---\n",
      "Before: ' \"command\" : IECore.'\n",
      "After: '( Gaffer.WeakMethod(')\n",
      "Expected: curry\n",
      "latency_ms: 1064.1157920472324\n",
      "LLM completions: [('curry', 0.993730874886289)]\n",
      "filter latency_ms: 510.28283406049013\n",
      "Correct! Pred:`curry` (99.4%)\n",
      "\n",
      "--- Sample 86, Valid Sample 59 ---\n",
      "Before: '(self, key, default='\n",
      "After: '):\\n        \"\"\"Pop th')\n",
      "Expected: _missing\n",
      "latency_ms: 1342.2910841181874\n",
      "LLM completions: [('_missing', 0.9396528500735334)]\n",
      "filter latency_ms: 506.1481245793402\n",
      "Correct! Pred:`_missing` (94.0%)\n",
      "\n",
      "--- Sample 87, Valid Sample 60 ---\n",
      "Before: 'x ) :\\n\\t\\tif not plug.'\n",
      "After: '().isSame( node ) :\\n')\n",
      "Expected: parent\n",
      "latency_ms: 759.1314590536058\n",
      "LLM completions: [('parent', 0.9033568672113051)]\n",
      "filter latency_ms: 511.6059170104563\n",
      "Correct! Pred:`parent` (90.3%)\n",
      "\n",
      "--- Sample 88, Valid Sample 61 ---\n",
      "Before: '                    '\n",
      "After: \"='''\\n    Name of the\")\n",
      "Expected: help\n",
      "latency_ms: 993.6416670680046\n",
      "LLM completions: [('help', 0.9509006729808629)]\n",
      "filter latency_ms: 508.67254193872213\n",
      "Correct! Pred:`help` (95.1%)\n",
      "\n",
      "--- Sample 89, Valid Sample 62 ---\n",
      "Before: 'MetadataDefinition( '\n",
      "After: ', \"Sequences include')\n",
      "Expected: \"fileSystemPathPlugValueWidget:includeSequenceFrameRange\"\n",
      "latency_ms: 2086.591416038573\n",
      "LLM completions: [('\"fileSystemPathPlugValueWidget', 0.8440301490079216)]\n",
      "filter latency_ms: 507.70979188382626\n",
      "Incorrect! Pred: `\"fileSystemPathPlugValueWidget` (84.4%) Label: `\"fileSystemPathPlugValueWidget:includeSequenceFrameRange\"`\n",
      "\n",
      "--- Sample 90, Valid Sample 63 ---\n",
      "Before: '    nid = db.Column('\n",
      "After: '.Text, primary_key=T')\n",
      "Expected: db\n",
      "latency_ms: 836.7027076892555\n",
      "LLM completions: [('db', 0.9885114499386789)]\n",
      "filter latency_ms: 512.6746250316501\n",
      "Correct! Pred:`db` (98.9%)\n",
      "\n",
      "--- Sample 91, Valid Sample 64 ---\n",
      "Before: \"ithm'],\\n            \"\n",
      "After: \": ipsecpolicy1['encr\")\n",
      "Expected: 'encryption_algorithm'\n",
      "latency_ms: 1195.2452082186937\n",
      "LLM completions: [(\"'encryption_algorithm'\", 0.9563152461363643)]\n",
      "filter latency_ms: 508.3271246403456\n",
      "Correct! Pred:`'encryption_algorithm'` (95.6%)\n",
      "\n",
      "--- Sample 92, Valid Sample 65 ---\n",
      "Before: 're = cache_property('\n",
      "After: ', None, bool)\\n    ma')\n",
      "Expected: 'no-store'\n",
      "latency_ms: 1537.519957870245\n",
      "LLM completions: [(\"'no\", 0.903756480020066)]\n",
      "filter latency_ms: 508.6424173787236\n",
      "Incorrect! Pred: `'no` (90.4%) Label: `'no-store'`\n",
      "\n",
      "--- Sample 93, Valid Sample 66 ---\n",
      "Before: 'ons = {\\n            '\n",
      "After: ': self.ipsecsiteconn')\n",
      "Expected: 'ipsec_site_connections'\n",
      "latency_ms: 1409.9837089888752\n",
      "LLM completions: [(\"'ipsec_site_connections'\", 0.745670218401701)]\n",
      "filter latency_ms: 0.0034170225262641907\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 94, Valid Sample 66 ---\n",
      "Before: 'ompute( self, plug, '\n",
      "After: \" ) :\\n\\n\\t\\t# we're allo\")\n",
      "Expected: context\n",
      "latency_ms: 646.9706250354648\n",
      "LLM completions: [('context', 0.9900618594947209)]\n",
      "filter latency_ms: 507.8041669912636\n",
      "Correct! Pred:`context` (99.0%)\n",
      "\n",
      "--- Sample 95, Valid Sample 67 ---\n",
      "Before: 'etlistdefault(self, '\n",
      "After: ', default_list=None)')\n",
      "Expected: key\n",
      "latency_ms: 813.8820002786815\n",
      "LLM completions: [('key', 0.9992675189857811)]\n",
      "filter latency_ms: 511.85858296230435\n",
      "Correct! Pred:`key` (99.9%)\n",
      "\n",
      "--- Sample 96, Valid Sample 68 ---\n",
      "Before: ')\\n\\n    def strftime('\n",
      "After: ', *args, **kwargs): ')\n",
      "Expected: self\n",
      "latency_ms: 770.9027091041207\n",
      "LLM completions: [('self', 0.995180935283209)]\n",
      "filter latency_ms: 510.57262532413006\n",
      "Correct! Pred:`self` (99.5%)\n",
      "\n",
      "--- Sample 97, Valid Sample 69 ---\n",
      "Before: '( 2 )\\n\\n\\t\\ts = Gaffer.'\n",
      "After: '()\\n\\t\\ts[\"plane\"] = Ga')\n",
      "Expected: ScriptNode\n",
      "latency_ms: 857.0640408433974\n",
      "LLM completions: [('Scene', 0.8270003685344266)]\n",
      "filter latency_ms: 508.713792078197\n",
      "Incorrect! Pred: `Scene` (82.7%) Label: `ScriptNode`\n",
      "\n",
      "--- Sample 98, Valid Sample 70 ---\n",
      "Before: 'er.query_expression.'\n",
      "After: ')\\n                  ')\n",
      "Expected: return_feature_types\n",
      "latency_ms: 1063.79562523216\n",
      "LLM completions: [('return_feature_types', 0.743448619503507)]\n",
      "filter latency_ms: 0.0034580007195472717\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 99, Valid Sample 70 ---\n",
      "Before: \"hLoginDomain': args.\"\n",
      "After: \".upper(), 'userName'\")\n",
      "Expected: domain\n",
      "latency_ms: 779.0800416842103\n",
      "LLM completions: [('domain', 0.9990922116021578)]\n",
      "filter latency_ms: 510.4581667110324\n",
      "Correct! Pred:`domain` (99.9%)\n",
      "\n",
      "--- Sample 100, Valid Sample 71 ---\n",
      "Before: 'LayoutItem ) ),\\n\\t\\t\\t\\t'\n",
      "After: '\\n\\t\\t\\t)\\n\\n\\t\\t_registerMe')\n",
      "Expected: parentItem\n",
      "latency_ms: 836.0111252404749\n",
      "LLM completions: [('None', 0.36407377783019684)]\n",
      "filter latency_ms: 0.0030831433832645416\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 101, Valid Sample 71 ---\n",
      "Before: \"    elif u'type' in \"\n",
      "After: ':\\n            out.wr')\n",
      "Expected: self\n",
      "latency_ms: 788.9629169367254\n",
      "LLM completions: [('self', 0.9995542368484039)]\n",
      "filter latency_ms: 510.39608381688595\n",
      "Correct! Pred:`self` (100.0%)\n",
      "\n",
      "--- Sample 102, Valid Sample 72 ---\n",
      "Before: 'format:\\n            '\n",
      "After: ' = \"{tmpdir}{sep}{uu')\n",
      "Expected: tmpname\n",
      "latency_ms: 790.1052501983941\n",
      "LLM completions: [('tmpname', 0.9570079251528324)]\n",
      "filter latency_ms: 505.151332821697\n",
      "Correct! Pred:`tmpname` (95.7%)\n",
      "\n",
      "--- Sample 103, Valid Sample 73 ---\n",
      "Before: 'firstNonPlugIndex = '\n",
      "After: '(\\n\\t\\t\\t( x[0] for x in')\n",
      "Expected: next\n",
      "latency_ms: 909.5157077535987\n",
      "LLM completions: [('next', 0.6953119759784826)]\n",
      "filter latency_ms: 0.0026668421924114227\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 104, Valid Sample 73 ---\n",
      "Before: ', port, cluster_id, '\n",
      "After: '):\\n    node = Proxy(')\n",
      "Expected: eru_container_id\n",
      "latency_ms: 935.7636668719351\n",
      "LLM completions: [('eru_container_id', 0.9949264562991286)]\n",
      "filter latency_ms: 513.4453340433538\n",
      "Correct! Pred:`eru_container_id` (99.5%)\n",
      "\n",
      "--- Sample 105, Valid Sample 74 ---\n",
      "Before: 'irectory() )\\n\\n\\t\\ta = '\n",
      "After: '.propertyNames()\\n\\t\\ts')\n",
      "Expected: p\n",
      "latency_ms: 777.7175828814507\n",
      "LLM completions: [('p', 0.9942091630065762)]\n",
      "filter latency_ms: 509.2847081832588\n",
      "Correct! Pred:`p` (99.4%)\n",
      "\n",
      "--- Sample 106, Valid Sample 75 ---\n",
      "Before: 'et_eula_status() is '\n",
      "After: ':\\n            print(')\n",
      "Expected: True\n",
      "latency_ms: 907.0788333192468\n",
      "LLM completions: [('True', 0.42475612694488346)]\n",
      "filter latency_ms: 0.0026659108698368073\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 107, Valid Sample 75 ---\n",
      "Before: ':\\n        rv = self.'\n",
      "After: '.pop(idx)\\n        se')\n",
      "Expected: _headers\n",
      "latency_ms: 723.7933329306543\n",
      "LLM completions: [('_headers', 0.9262398630287465)]\n",
      "filter latency_ms: 509.1327503323555\n",
      "Correct! Pred:`_headers` (92.6%)\n",
      "\n",
      "--- Sample 108, Valid Sample 76 ---\n",
      "Before: 'eturn (\\n            '\n",
      "After: ' in self or\\n        ')\n",
      "Expected: 'application/xhtml+xml'\n",
      "latency_ms: 1067.3889173194766\n",
      "LLM completions: [(\"'application\", 0.9085805439634008)]\n",
      "filter latency_ms: 507.446208037436\n",
      "Incorrect! Pred: `'application` (90.9%) Label: `'application/xhtml+xml'`\n",
      "\n",
      "--- Sample 109, Valid Sample 77 ---\n",
      "Before: '    cost, h1, h2 = ['\n",
      "After: ', b1_h, b2_h]\\n      ')\n",
      "Expected: 0.\n",
      "latency_ms: 767.0331667177379\n",
      "LLM completions: [('T', 0.5774837315618468)]\n",
      "filter latency_ms: 0.003000255674123764\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 110, Valid Sample 77 ---\n",
      "Before: '_bios_version(item, '\n",
      "After: '._errors, \\\\\\n        ')\n",
      "Expected: self\n",
      "latency_ms: 1371.2177500128746\n",
      "LLM completions: [('self', 0.9856137607097682)]\n",
      "filter latency_ms: 508.72054090723395\n",
      "Correct! Pred:`self` (98.6%)\n",
      "\n",
      "--- Sample 111, Valid Sample 78 ---\n",
      "Before: 'm})\\n        if self.'\n",
      "After: ':\\n            self.o')\n",
      "Expected: on_update\n",
      "latency_ms: 988.1677920930088\n",
      "LLM completions: [('on_update', 0.9983685451009486)]\n",
      "filter latency_ms: 513.3042908273637\n",
      "Correct! Pred:`on_update` (99.8%)\n",
      "\n",
      "--- Sample 112, Valid Sample 79 ---\n",
      "Before: 'it4task(task, tout, '\n",
      "After: '=verbose)\\n          ')\n",
      "Expected: verbose\n",
      "latency_ms: 784.4235408119857\n",
      "LLM completions: [('verbose', 0.9751203093315115)]\n",
      "filter latency_ms: 507.4995830655098\n",
      "Correct! Pred:`verbose` (97.5%)\n",
      "\n",
      "--- Sample 113, Valid Sample 80 ---\n",
      "Before: '          out.write('\n",
      "After: \")\\n\\n        if u'Help\")\n",
      "Expected: '\\n'\n",
      "latency_ms: 1011.8168750777841\n",
      "LLM completions: [(\"'\\\\n'\", 0.9971739276446924)]\n",
      "filter latency_ms: 509.7169582732022\n",
      "Correct! Pred:`'\\n'` (99.7%)\n",
      "\n",
      "--- Sample 114, Valid Sample 81 ---\n",
      "Before: '):\\n            info['\n",
      "After: \"] = 'file'\\n        e\")\n",
      "Expected: 'type'\n",
      "latency_ms: 936.4884169772267\n",
      "LLM completions: [(\"'type'\", 0.9389204222747687)]\n",
      "filter latency_ms: 508.1251668743789\n",
      "Correct! Pred:`'type'` (93.9%)\n",
      "\n",
      "--- Sample 115, Valid Sample 82 ---\n",
      "Before: ' not in data:\\n      '\n",
      "After: \" {'jsonrpc':'2.0',\\n \")\n",
      "Expected: return\n",
      "latency_ms: 787.6731669530272\n",
      "LLM completions: [('return', 0.9460433498543704)]\n",
      "filter latency_ms: 510.06462471559644\n",
      "Correct! Pred:`return` (94.6%)\n",
      "\n",
      "--- Sample 116, Valid Sample 83 ---\n",
      "Before: '               self.'\n",
      "After: '.append(newclass)\\n  ')\n",
      "Expected: _classes_registry\n",
      "latency_ms: 1252.3129577748477\n",
      "LLM completions: [('_classes_registry', 0.9221751743932948)]\n",
      "filter latency_ms: 507.059957832098\n",
      "Correct! Pred:`_classes_registry` (92.2%)\n",
      "\n",
      "--- Sample 117, Valid Sample 84 ---\n",
      "Before: ' 8 )\\n\\n\\t\\ts = sorted( '\n",
      "After: ', key=str )\\n\\t\\tself.a')\n",
      "Expected: c\n",
      "latency_ms: 768.4693331830204\n",
      "LLM completions: [('c', 0.7318861338334298)]\n",
      "filter latency_ms: 0.003541819751262665\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 118, Valid Sample 84 ---\n",
      "Before: \"equest.get('count', \"\n",
      "After: \"))\\n        request['\")\n",
      "Expected: '1'\n",
      "latency_ms: 648.4578340314329\n",
      "LLM completions: [('1', 0.652608480956015)]\n",
      "filter latency_ms: 0.002750195562839508\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 119, Valid Sample 84 ---\n",
      "Before: '( self.__addItem ), '\n",
      "After: ', IECore.BoolData( F')\n",
      "Expected: \"\"\n",
      "latency_ms: 770.197874866426\n",
      "LLM completions: [('\"\"', 0.32840074847385936)]\n",
      "filter latency_ms: 0.0033332034945487976\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 120, Valid Sample 84 ---\n",
      "Before: 'ata, dict):\\n        '\n",
      "After: ' = self.process(data')\n",
      "Expected: resdata\n",
      "latency_ms: 716.4387498050928\n",
      "LLM completions: [('resdata', 0.6399415088227071)]\n",
      "filter latency_ms: 0.0032079406082630157\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 121, Valid Sample 84 ---\n",
      "Before: 'atches:\\n            '\n",
      "After: ' client_item, qualit')\n",
      "Expected: for\n",
      "latency_ms: 973.3519582077861\n",
      "LLM completions: [('for', 0.7998086948001467)]\n",
      "filter latency_ms: 0.0029169023036956787\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 122, Valid Sample 84 ---\n",
      "Before: '\\tif not isinstance( '\n",
      "After: '.getPlug(), w.plugTy')\n",
      "Expected: self\n",
      "latency_ms: 776.2697502039373\n",
      "LLM completions: [('w', 0.7563254367333176)]\n",
      "filter latency_ms: 0.0027921050786972046\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 123, Valid Sample 84 ---\n",
      "Before: \" {'ikepolicy': self.\"\n",
      "After: '.first()}\\n        ne')\n",
      "Expected: api_ikepolicies\n",
      "latency_ms: 1199.2185418494046\n",
      "LLM completions: [('api_ikepolicies', 0.8326714701560242)]\n",
      "filter latency_ms: 511.88916712999344\n",
      "Correct! Pred:`api_ikepolicies` (83.3%)\n",
      "\n",
      "--- Sample 124, Valid Sample 85 ---\n",
      "Before: '    def index(self, '\n",
      "After: '):\\n        \"\"\"Return')\n",
      "Expected: header\n",
      "latency_ms: 997.5661667995155\n",
      "LLM completions: [('header', 0.9996808594195826)]\n",
      "filter latency_ms: 507.54645839333534\n",
      "Correct! Pred:`header` (100.0%)\n",
      "\n",
      "--- Sample 125, Valid Sample 86 ---\n",
      "Before: '  else:\\n            '\n",
      "After: ' id in ids:\\n        ')\n",
      "Expected: for\n",
      "latency_ms: 844.8999579995871\n",
      "LLM completions: [('for', 0.9852469144418451)]\n",
      "filter latency_ms: 505.51450019702315\n",
      "Correct! Pred:`for` (98.5%)\n",
      "\n",
      "--- Sample 126, Valid Sample 87 ---\n",
      "Before: 'ntation.Horizontal, '\n",
      "After: ' = 4 ) :\\n\\n\\t\\t\\t\\t\\tself.')\n",
      "Expected: spacing\n",
      "latency_ms: 785.8776249922812\n",
      "LLM completions: [('spacing', 0.9633153670340818)]\n",
      "filter latency_ms: 507.9718753695488\n",
      "Correct! Pred:`spacing` (96.3%)\n",
      "\n",
      "--- Sample 127, Valid Sample 88 ---\n",
      "Before: 'a.###.txt\" )\\n\\t\\tself.'\n",
      "After: '( str(s[1]), self.te')\n",
      "Expected: assertEqual\n",
      "latency_ms: 813.8742498122156\n",
      "LLM completions: [('assertEqual', 0.9949015584733789)]\n",
      "filter latency_ms: 509.5876660197973\n",
      "Correct! Pred:`assertEqual` (99.5%)\n",
      "\n",
      "--- Sample 128, Valid Sample 89 ---\n",
      "Before: 'ible( \"Node Graph\", '\n",
      "After: ' = True ) :\\n\\n\\t\\t\\t\\twit')\n",
      "Expected: collapsed\n",
      "latency_ms: 870.8664998412132\n",
      "LLM completions: [('collapsed', 0.9614841634549636)]\n",
      "filter latency_ms: 507.44941597804427\n",
      "Correct! Pred:`collapsed` (96.1%)\n",
      "\n",
      "--- Sample 129, Valid Sample 90 ---\n",
      "Before: '   return self._con.'\n",
      "After: \"(server['uri'] + '/r\")\n",
      "Expected: get\n",
      "latency_ms: 986.4013749174774\n",
      "LLM completions: [('get', 0.9980378578526321)]\n",
      "filter latency_ms: 506.98470836505294\n",
      "Correct! Pred:`get` (99.8%)\n",
      "\n",
      "--- Sample 130, Valid Sample 91 ---\n",
      "Before: '  return \"%s@%s\" % ('\n",
      "After: ', self._hostname)\\n\\n ')\n",
      "Expected: userid\n",
      "latency_ms: 1024.1698333993554\n",
      "LLM completions: [('userid', 0.9273344390130873)]\n",
      "filter latency_ms: 508.6475829593837\n",
      "Correct! Pred:`userid` (92.7%)\n",
      "\n",
      "--- Sample 131, Valid Sample 92 ---\n",
      "Before: '       if attrentry['\n",
      "After: \"].lower() == u'objec\")\n",
      "Expected: u'type'\n",
      "latency_ms: 1266.4221250452101\n",
      "LLM completions: [(\"u'type'\", 0.7444198851502225)]\n",
      "filter latency_ms: 0.003166031092405319\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 132, Valid Sample 92 ---\n",
      "Before: '\"\"\\n\\n    max_stale = '\n",
      "After: \"('max-stale', '*', i\")\n",
      "Expected: cache_property\n",
      "latency_ms: 1504.6937502920628\n",
      "LLM completions: [('cache_property', 0.9989112560631652)]\n",
      "filter latency_ms: 508.6131659336388\n",
      "Correct! Pred:`cache_property` (99.9%)\n",
      "\n",
      "--- Sample 133, Valid Sample 93 ---\n",
      "Before: '              key = '\n",
      "After: '.group(1)\\n          ')\n",
      "Expected: result\n",
      "latency_ms: 596.8697909265757\n",
      "LLM completions: [('result', 0.9990035697344128)]\n",
      "filter latency_ms: 511.1417076550424\n",
      "Correct! Pred:`result` (99.9%)\n",
      "\n",
      "--- Sample 134, Valid Sample 94 ---\n",
      "Before: '    return not self.'\n",
      "After: '(other)\\n\\n    def get')\n",
      "Expected: __eq__\n",
      "latency_ms: 1191.398334223777\n",
      "LLM completions: [('__eq__', 0.8628907569937578)]\n",
      "filter latency_ms: 507.61566683650017\n",
      "Correct! Pred:`__eq__` (86.3%)\n",
      "\n",
      "--- Sample 135, Valid Sample 95 ---\n",
      "Before: \"'qop'] = header_set.\"\n",
      "After: '()\\n        return pa')\n",
      "Expected: to_header\n",
      "latency_ms: 1005.9292921796441\n",
      "LLM completions: [('split', 0.1576894328445277)]\n",
      "filter latency_ms: 0.0032079406082630157\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 136, Valid Sample 95 ---\n",
      "Before: ' db.Column(db.Text, '\n",
      "After: '=True)\\n    nid = db.')\n",
      "Expected: primary_key\n",
      "latency_ms: 794.0609161742032\n",
      "LLM completions: [('primary_key', 0.8118627928455756)]\n",
      "filter latency_ms: 510.72049979120493\n",
      "Correct! Pred:`primary_key` (81.2%)\n",
      "\n",
      "--- Sample 137, Valid Sample 96 ---\n",
      "Before: '                    '\n",
      "After: ' : parameter.query_e')\n",
      "Expected: \"isPrivate\"\n",
      "latency_ms: 964.5810001529753\n",
      "LLM completions: [('\"private\"', 0.7516624745042656)]\n",
      "filter latency_ms: 0.0029578804969787598\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 138, Valid Sample 96 ---\n",
      "Before: '     if xmpp_read < '\n",
      "After: ':\\n            return')\n",
      "Expected: msg_count\n",
      "latency_ms: 896.1839582771063\n",
      "LLM completions: [('msg_count', 0.3875993127218325)]\n",
      "filter latency_ms: 0.0024586915969848633\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 139, Valid Sample 96 ---\n",
      "Before: ' text )\\n\\n\\tdef hash( '\n",
      "After: ', context ) :\\n\\n\\t\\th =')\n",
      "Expected: self\n",
      "latency_ms: 612.2932499274611\n",
      "LLM completions: [('self', 0.9972055779222659)]\n",
      "filter latency_ms: 513.8383340090513\n",
      "Correct! Pred:`self` (99.7%)\n",
      "\n",
      "--- Sample 140, Valid Sample 97 ---\n",
      "Before: \"\\n            'dpd': \"\n",
      "After: \"['dpd'],\\n           \")\n",
      "Expected: ipsecsiteconnection1\n",
      "latency_ms: 1430.1552502438426\n",
      "LLM completions: [('ipsecsiteconnection1', 0.9364787863728503)]\n",
      "filter latency_ms: 509.32316668331623\n",
      "Correct! Pred:`ipsecsiteconnection1` (93.6%)\n",
      "\n",
      "--- Sample 141, Valid Sample 98 ---\n",
      "Before: '             msgbox.'\n",
      "After: '()\\n                e')\n",
      "Expected: GetUnreadMsg\n",
      "latency_ms: 1177.7357091195881\n",
      "LLM completions: [('LoadMsgText', 0.22380859096988115)]\n",
      "filter latency_ms: 0.0029578804969787598\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 142, Valid Sample 98 ---\n",
      "Before: '\"\"\\n        d = dict('\n",
      "After: ')\\n        auth_type ')\n",
      "Expected: self\n",
      "latency_ms: 827.0170828327537\n",
      "LLM completions: [('self', 0.8316874953079115)]\n",
      "filter latency_ms: 510.67950017750263\n",
      "Correct! Pred:`self` (83.2%)\n",
      "\n",
      "--- Sample 143, Valid Sample 99 ---\n",
      "Before: 'bs({neutronclient: ('\n",
      "After: ',)})\\n    def test_ip')\n",
      "Expected: 'show_ipsecpolicy'\n",
      "latency_ms: 1477.9982911422849\n",
      "LLM completions: [(\"'list_ipsecpolicies'\", 0.6313033949526624)]\n",
      "filter latency_ms: 0.0029997900128364563\n",
      "No completions above 80.0% threshold. Skipping sample...\n",
      "\n",
      "--- Sample 144, Valid Sample 99 ---\n",
      "Before: '     query = self.E.'\n",
      "After: \"({ 'xmlns': __disco_\")\n",
      "Expected: query\n",
      "latency_ms: 687.887417152524\n",
      "LLM completions: [('query', 0.9721024045732676)]\n",
      "filter latency_ms: 512.6576670445502\n",
      "Correct! Pred:`query` (97.2%)\n",
      "\n",
      "--- Sample 145, Valid Sample 100 ---\n",
      "Before: ':\\n            z_t = '\n",
      "After: '.tanh(T.dot(X,W[:,:n')\n",
      "Expected: T\n",
      "latency_ms: 640.8657501451671\n",
      "LLM completions: [('T', 0.9257460469646266)]\n",
      "filter latency_ms: 511.9325420819223\n",
      "Correct! Pred:`T` (92.6%)\n",
      "\n",
      "Accuracy over 100 samples: 93.0% | Coverage: 69.0% | Prob TH: 80.0%: 93.0% \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(20)\n",
    "\n",
    "MODEL_PATH = \"./models/qwen2.5-coder-3b-instruct-q4_k_m.gguf\"\n",
    "# MODEL_PATH = \"./models/deepseek-coder-1.3b-instruct.Q4_K_M.gguf\"\n",
    "BASE_DIR = \"./data/py150_files\"\n",
    "INPUT_FILE = \"python100_eval.txt\"\n",
    "OUTPUT_FILE = \"./data/eval_tokens.json\"\n",
    "SAMPLES_FILE = \"./data/samples.json\"\n",
    "\n",
    "STOP_TOKENS = [\n",
    "    '(', ')', '[', ']', '{', '}',  # Brackets\n",
    "    ',', ':', ';',                   # Delimiters  \n",
    "    '.', \n",
    "    '+', '-', '*', '/', '%', '@',   # Arithmetic\n",
    "    '=', '<', '>', '!',              # Comparison/Assignment\n",
    "    '&', '|', '^', '~',              # Bitwise\n",
    "    '\\n', '\\t', ' ',\n",
    "    \"<|endoftext|>\"\n",
    "]\n",
    "\n",
    "dataloader = DataLoader(basedir=BASE_DIR, infile=INPUT_FILE, outfile=OUTPUT_FILE)\n",
    "data = dataloader.get_data()\n",
    "\n",
    "sample_generator = SampleGenerator(basedir=BASE_DIR, samples_file=SAMPLES_FILE, n_before=100, n_after=100, one_per_file=False)\n",
    "samples = sample_generator.get_samples(data)\n",
    "random.shuffle(samples)\n",
    "\n",
    "model = LocalCodeModel(model_path=MODEL_PATH,\n",
    "                       max_tokens=10,\n",
    "                       n_threads=8,\n",
    "                       n_gpu_layers=-1,\n",
    "                       stop_tokens=STOP_TOKENS,\n",
    "                       n_batch=512)\n",
    "\n",
    "lsp = LSPClient(cmd=['pylsp']) # start python language server\n",
    "\n",
    "evaluator = CompletionEvaluator(model=model, lsp=lsp, basedir=BASE_DIR)\n",
    "\n",
    "n_samples = 100\n",
    "accuracy = evaluator.evaluate(samples=samples, prob_threshold=0.8, n=n_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
